{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense, Dropout\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Set to -1 if CPU should be used CPU = -1 , GPU = 0\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\ncpus = tf.config.experimental.list_physical_devices('CPU')\n\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)\nelif cpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the image size with are learning from\nIMG_WIDTH, IMG_HEIGHT = 224, 224\n\n# Set the constants\nTRAIN_DATA_DIR = '../input/animals/train'\nVALIDATION_DATA_DIR = '../input/animals/validation'\n\nEPOCHS = 15\nBATCH_SIZE = 100\nNUM_CLASSES = 16\n\n# Machine Learning Model Filename\nMODEL_FILENAME = './saved_model.h5'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    # to check if first dim is channel or not\n    if K.image_data_format() == 'channels_first':\n        input_shape = (3, IMG_WIDTH, IMG_HEIGHT)\n    else:\n        input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)\n    \n    # start to build architecture of model\n    model = Sequential()\n    \n    model.add(tf.keras.applications.EfficientNetB2(\n        input_shape=input_shape,\n        include_top=False,\n        weights=\"imagenet\",\n        classes=NUM_CLASSES,\n        classifier_activation=\"softmax\"\n    ))\n\n    model.add(Flatten())\n    model.add(Dense(128))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n    \n    # make training configuration\n    model.compile(loss='binary_crossentropy', \n                  optimizer=tf.keras.optimizers.SGD(lr=0.01), \n                  metrics=['accuracy']\n                 )\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model):\n    # data augmentation for training\n    train_datagen = ImageDataGenerator(\n        rescale=1.0 / 255,\n        validation_split=0.1,\n        rotation_range=30,\n        shear_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n\n    train_generator = train_datagen.flow_from_directory(\n        TRAIN_DATA_DIR,\n        target_size=(IMG_WIDTH, IMG_HEIGHT),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        subset='training',\n        shuffle=True\n    )\n\n    validation_generator = train_datagen.flow_from_directory(\n        directory=TRAIN_DATA_DIR,\n        target_size=(IMG_WIDTH, IMG_HEIGHT),\n        batch_size=BATCH_SIZE,\n        class_mode=\"categorical\",\n        subset='validation',\n        shuffle=True,\n    )\n    \n    # start training\n    model.fit(\n        train_generator,\n        validation_data=train_generator,\n        steps_per_epoch = train_generator.n//BATCH_SIZE,\n        validation_steps = validation_generator.n//BATCH_SIZE,\n        epochs=EPOCHS,\n        workers=2\n    )\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model):\n\n    model = tf.keras.models.load_model('./saved_model.h5')\n\n    test_datagen = ImageDataGenerator(rescale=1. / 255)\n\n    test_generator = test_datagen.flow_from_directory(\n    VALIDATION_DATA_DIR,\n    target_size=(IMG_WIDTH, IMG_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode=\"categorical\",\n    )\n\n    loss, accuracy = model.evaluate_generator(test_generator)\n    print('Test loss:', loss)\n    print('Test accuracy:', accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    myModel = build_model()\n    myModel = train_model(myModel)\n    myModel.save(MODEL_FILENAME)\n    evaluate(myModel)\n    \nmain()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}